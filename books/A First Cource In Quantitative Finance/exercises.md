# Chapter 2
## 2.1
Consider a simplified version of the wheel of fortune as seen in the figure. Consider that the wheel is fair. Create a complete probability space as a model for one turn of the wheel
![Alt text](./pictures/exercise2.1.jpg)

### Solution
A probability space needs a sample space $\Omega$, a $\sigma$-algebra $\mathcal{F}$ and the probability measure $P$.
Here we have 
$$
\Omega = {1,2,3},
$$
$$
\mathcal{F}={\emptyset, {1}, {2}, {3}, {1, 2}, {1,3}, {2,3}, \Omega}
$$
$$
P(\{\omega\})=\begin{cases}
\frac{1}{4} & \text{for } \omega \in \{1,2\} \\
\frac{1}{2} & \text{for } \omega = 3
\end{cases}
$$

## 2.2
Calculate the natural filtration from exercise 2.1, generated by the outcome $A=\{2\}$. Does $\mathcal{F}_1 = 2^\Omega$ hold?

Before any turns the $\sigma$-algebra is
$$
\mathcal{F} = \{\emptyset, \Omega\}
$$

after the outcome $A=\{2\}$ it is
$$
\mathcal{F}_1 = \{\emptyset, \{2\}, \{1, 3\} \Omega\}
$$

Does $\mathcal{F}_1 = 2^\Omega$ hold? The generated $\mathcal{F}_1$ is a subset of the power set. $\mathcal{F}_1 = 2^\Omega$ is the smallest $\sigma$-algebra contaning the outcome.

## 2.3
Consider rolling a fair die with $X(\omega)$ as the number of pips and the event $A$ of throwing an even number. Show that the conditional expectation, given $A$ is greater than the unconditional expectation.

Unconditional probability measure
$$
P(\omega)=\frac{1}{6}
$$
The expectation value is given
$$
E[X] = \sum_{n=1}^6 \frac{1}{6} = \frac{21}{6} = 3.5  
$$

For the conditional probability given $A$
$$
E[X]_A = \sum_{n=1}^6 n P(X=n \mid A)=\sum_{n=1}^{6} n \frac{P(X\cap A)}{P(A)}
$$
where
$$
\frac{P(X\cap A)}{P(A)} = \begin{cases}
\frac{1}{3} & \text{for n even} \\
0 & \text{for n odd}
\end{cases}
$$
using this probability measure to get
$$
E[X]_A = \sum_{n=1}^3 \frac{2n}{3} + \sum_{n=1}^{3}(2n-1)\cdot 0 = \frac{12}{3}=4
$$

## 2.4
Considering the die example of exercise 2.3. Show that the property
$$
E[X]=E[E[X\mid A]]
$$
holds for $A$ being the event of throwing an even number.

$$
E[E[X\mid A]] = E[X\mid A] P(A) + E[X\mid A^C] P(A^C) = 4 \frac{1}{2} + 3 \frac{1}{2}=\frac{7}{2}=3.5
$$

## 2.5
A theorem by Kolmogorov states  that every stochastic process $X(t)$ which satisfies the inequality 
$$
E[\lvert X(t) - X(s) \rvert^a] \le c \lvert t-s \rvert^{1+b}
$$
for $t\gt s$ and a particular set of numbers $a$, $b$ and $c$, has almost surely continuous paths. Show that the Wiener process meets this condition. Use the moment structure of normally distributed random variables (2.31) on page 19.

From the definition of the Wiener process i know that the variance between two times is $t-s$. Considering the moment structure in (2.31) we see that
$$M_2=\lvert t-s \lvert$$ 
which match Kolmogorov for $a=2$, $b=0$ and $c=1$

## 2.6
Assume $N\sim \text{Poi}(\lambda)$ is a Possion-distributed random variable with probability mass function 
$$
f(n)=e^{-\lambda}\frac{\lambda^n}{n!}
$$
for $n\in \mathbb{N}_0$. Consider a random variable $X$, with
$$
X=\sum_{n=0}^N X_n
$$
where $X_n$ are independent an identically distributed random variables. Prove that the relation 
$$
\varphi(u)=\text{exp}[\lambda(\varphi_n(u)-1)]
$$
holds for the characteristic of $X$ and $X_n$. Use the one to one correspondence of conditional probability and conditional distribution functions.

Notice that it is the sum limit $N$ that is passion distributed.
$$
\varphi(u) = E[e^{iuX }]
$$


