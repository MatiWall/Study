
# Propability and Measure

A family $\mathcal{F}$ of sets (events) $A_1, A_2,...$ is called a $\sigma$-algbra if it satisfy the following conditions
1. $\mathcal{F}$ is non empty
2. if $A \in \mathcal{F}$ then $A^c \in \mathcal{F}$
3. if $A_1, A_2, ... \in \mathcal{F}$ then $\bigcup_{n=0}^\infty A_n \in \mathcal{F}$

>**$\sigma$-algebra** is used to ensure consistency when defining measures on a collection of events or sets.

<div style="color: red">go back and take notes from begining untill equation 2.4 (first quick calculation)</div>

> **Power Set** - The power set $2^{\Omega}$ is the set of all subesets of $\Omega$ that can be constructed. There are $2^{\#\Omega}$ possible subsets.

For uncountable infinite set like $\Omega = \mathbb{R}$ is too large, instead one uses the $\sigma-\text{algebra}$ generated by all open intervals $(a, b)$ in \mathbb{R} for $a<b$. This is known as the Borel-$\sigma$-algebra ($\mathcal{B}(\mathbb{R})$). From the definition of the $\sigma$-algebra this containe way more than just he open intervals. All sets than can be generated from open, half open and closed intervals are in the Bores-$\sigma$-algebra. For example sets like $(a,c)^C = (-\infty, a] \cup [b, \infty)$ and the set generated by 
$$ \bigcap_{n=1}^\infty (a-\frac{1}{n}, b+\frac{1}{n})=[a, b] $$
Here for every point chosen arbitary close to the left of $a$ or to the right of $b$ an $n$ can be chosen such that this point in not in the set i.e. it becomes a closed set from $a$ to $b$.

> **Why intersection and not Limit**
> Limit is not a valid approch in set theory.

> **Open and Closed Sets** A set is oppen if for every point in the set there is a small negboring region that is also in the set. For example $(a,b)$ is an open aset as it includes all points between $a$ and $b$ and the endpoints are not included. Also $x\gt5$ is an open set whereas $x\ge 5$ is a closed set.

Consider the $\sigma$-algebra generated by the throw of a die where we are only interested in wether the number on top is even or odd. The sigma algebra generated by this is $$\mathcal{F} = \{\emptyset, A, A^C, \Omega\}$$ where $A = \{2, 4, 6\}$ and $A^C = \{1,3,5\}$. In general a $\sigma$-algebra generated by the event $A$ is $\mathcal{F} = \{\emptyset, A, A^C, \Omega\}$ also denoted $\mathcal{F}=\sigma(A)$, which is the smallest $\sigma$-algebra containint $A$.

A function $\mu: \mathcal{F} \rightarrow \mathbb{R}^+_0$ with properties:
1. $\mu(\emptyset) = 0$
2. $\mu \left(\bigcup_{n=1}^m A_n \right) = \Sigma_{n=1}^m \mu(A_n)$ for $A_1, A_2,... \in \mathcal{F}$ and $A_i \cap A_j = \emptyset$ for $i\neq j$

> Note: $\mathbb{R}^+_0$  represents the set of all non negative numbers $\{x\in\mathbb{R}\mid x \ge 0\}$. It could also have been like $\mathbb{R}^+_2$ which would be $\{x\in\mathbb{R}\mid x \ge 2\}$

$\mu$ is called a measure on $(\Omega, \mathcal{F})$. The triplet $(\Omega, \mathcal{F}, \mu)$ is called a measure space (sometimes also proppability triplet). A measure is the most natural concept of a length assigned to all sets in the $\sigma$-algebra.
> Note: Consider the measurable space $(\mathbb{R}, \mathcal{B})$ with the Borel-$\sigma$-algebra generated by all half open intervals $(a, b]$ for $a \le b$ and choosing the Lebesgue-measure $\mu((a, b]) = b-a$

In propaility theory one assigns the overall length $\mu(\Omega) = 1$ to $\Omega$. The associated measure is called probability and is abbreviated $P(A)$ for $A \in \mathcal{F}$. The triplet $(\Omega, \mathcal{F}, P)$ is called the probability space.

# Filtrations and the Flow of Information 
Consider tossing a coin two times in succession. The outcomes can be labeled $$\Omega = \{(H, H), (H, T), (T, H), (T, T)\}$$. Inventing the variable $t$ to track occurred number of tosses this can take values $t \in \{0, 1, 2\}$. We then ask, what is the $\sigma$-algebra $\mathcal{F}_t$ generated by the coin tossing at $t$. At $t=0$ $$\mathcal{F}_0 = \{\emptyset, \Omega\}$$ 
> **Empty set and Sample Space**: The empty set and sample space are included in the $\sigma$-algebra for consistency. It ensures that the $\sigma$-algebra is closed under complement and countable unions. $\sigma$-algebras are used to structure the sets on which you can define measures or probabilities. Including the empty set and the sample space ensures that the $\sigma$-algebra adheres to the necessary mathematical properties and can be used consistently in probability and measure theory.

Assuming that the first toss is $H$ we know that the event $A=\{(H, H), (H, T)\}$ has happened at $t=1$. Adding the required unions and complement to obtains $$ \mathcal{F}_1 = \{\emptyset, \{(H, T), (H, H)\}, \{(T, T), (T, H)\}, \Omega\}$$

>**Complement**: Taking the complement means creating a new set with all the elements that are not in the original set. <br> 
Given a universal set $U$ and a subset $A \subseteq U$ the complement is given $$A^C = \{x \in U \mid x \notin A\}$$ i.e. $A^c$ consists of all elements not in $A$. <br>
In probability theory $P(A^c)=1-P(A)$ 

By comparing $\mathcal{F}_0$ and $\mathcal{F}_1$ we see that the finer the partition of the $\sigma$-algebra the more information is revealed by the history of the process. Also $\mathcal{F}_0 \subset \mathcal{F}_1$ indicating that no information is forgotten. $\mathcal{F}_t$ captures the information up until $t$ and $\mathcal{F}_{t-1} \subset \mathcal{F}_t$ indicated that no information is lost.  

Considering the final toss we are able to say exactly which events happened or not like $(T, T)$ or $(H, T)$ i.e. $\mathcal{F}_2$ has the finest possible partition structure. Generating the $\sigma$-algebra by taking countable unions and complements you get the power set $$\mathcal{F}_2 = 2^\Omega$$


<div style="color: red">Please go through the above and make sure you understand. Unsure how a sigma algebra is generated</div>

# Conditional Probability
Consider the probability space $\mathcal{F}_A=\{A\cap B: B \in \mathcal{F}\}$ i.e. the family of all intersections of $A$ with all events in $\mathcal{F}$ 

Consider a mesurable space $(\Omega, \mathcal{F})$ for a six sided die and $\mathcal{F}=2^\Omega$. Let $A = \{2, 4, 6\}$ be the event for throwing an even number. Which events are included in $\mathcal{F}_A$ and why is it a $\sigma$-algebra. The intersection of $A$ with all other events in $\mathcal{F}$ generates the following family of sets
$$\mathcal{F}_A = \{\emptyset, \{2\}, \{4\}, \{6\}, \{2, 4\}, \{2, 6\}, \{4, 6\}, A\} $$
we see that $\mathcal{F}_A$ is itself the power set of $A$ and there for a $\sigma$-algebra.

If $P(A)>0$ the conditional probability $P(B\mid A)$ is defined as 
$$P(B\mid A) = \frac{P(B \cap A)}{P(A)}$$
The triplet $(A, \mathcal{F}_A, P(\cdot \mid A)$ forms a new probability space.

**Example** Take the measurable space $(\Omega, \mathcal{F})$ for a six sided die and the probability measure $$P(B) = \frac{\#B}{6}$$ for all $B \in \mathcal{F}$. Chose an event $A = \{2, 4, 6\}$ what are the conditional probabilities $P(A\mid A), P(\{2\}\mid A)$ and $P(\{5\}\mid A)$? First notice that $P(A)=\frac{1}{3}$ to obtain
$$P(A \mid A)= \frac{A \cap A}{P(A)} = \frac{P(A)}{P(A)}=1$$
$$P(\{2\} \mid A)= \frac{\{2\} \cap A}{P(A)} = \frac{P(\{2\})}{P(A)}=\frac{\frac{1}{6}}{\frac{1}{3}}=\frac{1}{2}$$
$$P(\{5\} \mid A)= \frac{\{5\} \cap A}{P(A)} = \frac{P(\emptyset)}{P(A)}=0$$
> The conditional probability generates a new measurable space in which the event is evaluated under the condition $A$.  In this space, the outcomes are restricted to those in $A$, and the probabilities are recalibrated based on this restriction. 

An immediate consequence of the definition of conditional probability is Bayes' rule
$$P(B\mid A)=\frac{P(A\mid B)P(B)}{P(A)}=\frac{P(A \mid B) P(B)}{P(A \mid B) P(B)+P(A \mid B^C) P(B^C)}$$

## Example: Medical Test and Bayes' Rule

### Problem:
Suppose there is a disease that affects 1% of a population. A test exists for this disease, and it has the following properties:
- **True positive rate (sensitivity)**: 90% (if you have the disease, the test is positive 90% of the time).
- **False positive rate**: 5% (if you donâ€™t have the disease, the test is positive 5% of the time).

Given that a person tests positive, we want to find the probability that the person actually has the disease.

### Step 1: Define the events
- $ D $: The event that the person has the disease.
- $ D^C $: The event that the person does not have the disease.
- $ T^+ $: The event that the test result is positive.

We want to find $P(D \mid T^+)$, the probability that the person has the disease given a positive test result.

### Step 2: Apply Bayes' Rule
Bayes' Rule is given by:

$$
P(D \mid T^+) = \frac{P(T^+ \mid D) P(D)}{P(T^+)}.
$$

Where:
- $ P(T^+ \mid D)$ is the probability that the test is positive given that the person has the disease (this is the sensitivity, 90% or 0.9).
- $P(D)$ is the prior probability of having the disease (1% or 0.01).
- $P(T^+)$ is the total probability of testing positive, which can be calculated using the law of total probability:

$$
P(T^+) = P(T^+ \mid D) P(D) + P(T^+ \mid D^C) P(D^C).
$$

Here:
- $P(T^+ \mid D^C)$ is the probability of testing positive given that the person does not have the disease (the false positive rate, 5% or 0.05).
- $P(D^C)$ is the probability of not having the disease (99% or 0.99).

### Step 3: Calculate $P(T^+)$
$$
P(T^+) = (0.9 \times 0.01) + (0.05 \times 0.99) = 0.009 + 0.0495 = 0.0585.
$$

### Step 4: Apply the formula
Now, apply Bayes' Rule to compute $ P(D \mid T^+) $:

$$
P(D \mid T^+) = \frac{0.9 \times 0.01}{0.0585} = \frac{0.009}{0.0585} \approx 0.1538.
$$

### Interpretation:
Given that a person tests positive, the probability that they actually have the disease is about **15.38%**. Despite the positive test result, the probability is still relatively low due to the low prevalence of the disease (1%) and the non-zero false positive rate.

## Probability Independence
In the case of independent probabilities we can visualize it as a rectangle (Fig 2.2), i.e. orthogonal sides (orthogonality is equivalent to independence). The Lebesgue-measure for the rectangle $A\cap B$ is $\mu(A \cap B)=\mu[A] \mu(B)$ i.e. the area. Analogously 
$$P(A \cap B)=P(A)P(B)$$
In the case of independence condtional probabilities collapse to unconditional probabilities
$$P(A \mid B)= \frac{P(A \cap B)}{P(B)}=\frac{P(A)(B)}{P(B)}=P(A)$$

# Random Variables and Stochastic Processes
So far we have seen a very elegant and rigorous model fra random experiments which is not very practical. Luckily it is possible to map the measurable space $(\Omega, \mathcal{F})$ onto another measurable space $(E, \mathcal{B})$ equipped with a distribution function induced by the original probability $P$. The link is established by a random variable or process.
> The designation random variable is a misnomer as it is actually a function $X: \Omega \rightarrow E$

In the case of a coin toss one can define following random variable 
$$
X(\omega)=
\begin{cases}
1 & \text{for } \omega = H \\
0  & \text{for } \omega = T
\end{cases}
$$
This link is only meaningful if the inverse mapping also exists $$X^-1 (B) = \{\omega \in \Omega : X(\omega) \in B\}$$. This is trivially fulfilled in the one to one mapping of the coin example.

> Point of the above is that a random variable maps from the measurable $\Omega$ to some other measurable space $E$ space, and that it is often easier to do calculations in $E$.

Observing a family of random variables $X_t(\omega)$ labeled by a continuous or discrete index set $0\le t \le T$, there also a family of $\sigma$-algebras $\mathcal{F}_t$ induced by $X^{-1}$ in the original probability space. This is the concept of filtration. The family $X(\omega)$ is called a stochastic process. If the filtration $\mathcal{F}_t$ is generated by $X_t$ it is called the natural filtration and if the process $X_t$ is measurable with respect to $\mathcal{F}_t$ it is called adapted to this $\sigma$-algebra.

## Example 2.5 (Wiener Process)
The stochastic process $W_t$ characterized by
1. $W_0 = 0$
2. $W_t$ has independent increments
3. $W_t - W_s \sim N(0, t-s)$ for $0 \le s \le t$

is called a Wiener process (Brownian Motion) and is found all over quantitative finance. $N(0, t-s)$ represents the normal distribution with mean $0$ and variance $t-s$. For a given time interval $t-s$ $W$ is a continuous random variable density function 
$$
    f(w) = \frac{1}{\sqrt{2 \pi (t-s)}}e^{-\frac{1}{2}\frac{w^2}{t-s}}
$$

The corresponding distribution function is obtained by integration (as continuous) 
$$
F(w) = \int_{-\infty}^w f(x) \text{d}x
$$

A subtle feature of continuous random variables is the fact that a single point has probability zero. 

<hr style="border-top: 0.1px dashed;">

A technical consequence of this uncountable feature is not non empty spaces can have zero probability. They don not even have to be small. the set of all rational numbers has zero probability. Such a set is called a null set. A probability space is called complete if all subsets of null sets are included in $\mathcal{F}$. It is possible to include these subsets because most statements concern events with probability larger than zero. This is indicated by the phrase "almost surely". For example the Wiener process as almost surely continuous but non-differential paths i.e. this property is at most violated at events with zero probability.

# Moments of Random Variables
Even though probability distributions are sufficient to describe random variables they are not very descriptive. Moments are some additional feature to characterize additional features. The fist moment is the expectation value defined in discrete and continuous case as
$$
    E[X] = \sum_n x_n f(x_n) \ \text{or} E[X] = \int x f(x) \text{d}x
$$

The expectation value is defined as the first raw moment, the second moment is usually a central moment i.e. the moment around the expectation value called variance. It is defined as
$$
Var[X] = E[(X-E[X])^2]=E[X^2] - E[X]^2
$$

> Cental moment is a moment taking around a value, often mean, and raw moment is without a reference value. This also means that for $\mu=0$ raw and central moments are identical.

The moments of a normal distribution are defined
$$
M_k = \int_{-\infty}^\infty (y-\mu)^k\frac{1}{\sqrt{2\pi\sigma}}e^{-\frac{1}{2}\left( \frac{y-\mu}{\sigma}\right)^2} \text{d}y = 
\begin{cases}
0 & \text{for odd} \\
(k-1)!!\sigma^k  & \text{for even}
\end{cases}
$$
All odd moments disappear as these describe asymmetry. The standardized third moment is called skewness. Even moments are related to the proportion of the probability mass located in the tails of the distribution. The standardized fourth moment is called kurtosis and is $3$ for a normal distribution. For most financial return time series it is between $6$ and $9$, indicating a more heavy tailed distribution than normal.

A related concept is that of mixed moments, the most prominent here being covariance. For two random variables $X$ and $Y$ the covariance is defined
$$
Cov[X, Y] = E[(X - E[X])(Y - E[Y])] = E[XY] - E[X]E[Y]
$$

Covariance is a linear measure between two random variables $X$ and $Y$ as the expectation value is a linear function. If two random variables has covariance zero it does not necessarily mean that they are independent.

## Example 2.5
Consider $X \sim N(0,1)$ and $Y = X^2$. Obviously these are highly dependent but what is there covariance.
$$
\text{Cov}[X, Y] = E[X Y] -E[X]E[Y] = E[X^3] - E[X]E[X^2]=0
$$

<hr style="border-top: 0.1px dashed;">

If on the other hand two random variables are independent their covariance is guarantied to vanish. Only in the case of normally distributed random variables are vanishing and independence equivalent. It is often more intuitive to use a rescaled version called correlation
$$
\rho_{XY} = \frac{\text{Cov}[X,Y]}{\sqrt{\text{Var}[X] \text{Var}[Y]}}
$$
Correlation and covariance are in one to one correspondence i.e. uncorrelatied also means zero covariance.

# Characteristic Function and Fourier-Transform